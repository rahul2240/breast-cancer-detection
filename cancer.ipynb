{"cells":[{"metadata":{"_uuid":"8169af97a70080589e016fc9a218ae3b527d9f2f"},"cell_type":"markdown","source":"**IMPORT FILES**"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ee3c9d52a96ca46ccefee28c856f176e7fdc5afa"},"cell_type":"markdown","source":"**Local Directory**"},{"metadata":{"_uuid":"4c804a48c43d5eafdd25f3544af52c495b5062dd"},"cell_type":"markdown","source":"We explore the name of the directory inside which our datafiles are present."},{"metadata":{"_uuid":"767ce7de7da5c8fb6dda190fc704f4917a26a0e8","trusted":true},"cell_type":"code","source":"print(os.listdir(\"../input/\")) #to check the name of the directory inside which we have our files","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"20071dff7c1d1a2d5380b4a7cdec66e5e48193da"},"cell_type":"markdown","source":"**Data Exploration**"},{"metadata":{"_uuid":"e4b4d6d49239abff9afaa7cf98572e4d7b7002c1"},"cell_type":"markdown","source":"\nIn data exploration we will first check the name of the files."},{"metadata":{"trusted":true,"_uuid":"7cc62d80286b3143be33c3670ee27f3a4ea48807"},"cell_type":"code","source":"from glob import glob #glob is used to extract the files from a particular folder\nfiles = glob('../input/breast-histopathology-images/**/*', recursive=True) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9a6ff8e9a26de0259ff210224282e48b9b3e21f"},"cell_type":"code","source":"print(files[1]) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"216bff6d7a7b509a679d0de79810938798c9f6d6"},"cell_type":"code","source":"extention=list() #will store end 3 letters of all the file names (extentions)\nfor image in files:\n    ext=image[-3:]\n    if ext not in extention:\n        extention.append(ext)\nalpha_ext=list()\nfor ex in extention: #any valid image will have extention in alphabets \n    if ex.isalpha() == True: #this line checks for such alphabet extentions\n        alpha_ext.append(ex)\nprint(alpha_ext)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e1cf5112f748df4a510daccab05d0e5d5f75a7a"},"cell_type":"markdown","source":"> **Code Conclusion :**  There are only png extentions which are present in alphabets therefore it means that we have only one image extention files with *.png* extentions. Therefore we will load only that."},{"metadata":{"_uuid":"a8b8fdf7fb4e8f4fcd2fe3e079fdb15f007f710c","trusted":true},"cell_type":"code","source":"from glob import glob\nData = glob('../input/breast-histopathology-images/**/*.png', recursive=True)  #we extract only png files","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ab36669bf059f1cf8087e54e29bde7a2b5068c0b","trusted":true},"cell_type":"code","source":"#del(files) #We don't need the files variable , so delete it.\nprint(len(Data))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"15f3df0714e745ec1334e864c6b22656b7f72a74"},"cell_type":"markdown","source":"> **Code Conclusion **: We have total of 277524 image files"},{"metadata":{"_uuid":"fdf8eb91f05d6230296955c0159788d5d6bcf676"},"cell_type":"markdown","source":"Next Step is that we will check whether the dimentions of all the images are same or different"},{"metadata":{"trusted":true,"_uuid":"e25ed3d7e736189164f489b7cec90194c4b704bd"},"cell_type":"code","source":"\nfrom PIL import Image #adds support for opening, manipulating, and saving many different image file formats\nfrom tqdm import tqdm #adds progress bar for the loops\ndimentions=list()\nx=1\nfor images in (Data):\n    dim = Image.open(images)\n    size= dim.size\n    if size not in dimentions:\n        dimentions.append(size)\n        x+=1\n    if(x>3): #going through all the images will take up lot of memory, so therefore we will check until we get three different dimentions.\n        break\nprint(dimentions)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"471190e7321837531cabbdbd747b30ab7e90d493"},"cell_type":"markdown","source":"> ***Code Conclusion : *** We can see that the dimentions of images are not equal therefore we would make it all equal  to work bettter with our network."},{"metadata":{"_uuid":"e1aaefe53764b6211c7555f0dfb1407bc246afc6"},"cell_type":"markdown","source":"***Data Extraction and Visualization***"},{"metadata":{"_uuid":"3e5cb6052974c45baa373bc359fd9cf3de037dfa","trusted":true},"cell_type":"code","source":"import cv2 #used for computer vision tasks such as reading image from file, changing color channels etc\nimport matplotlib.pyplot as plt #for plotting various graph, images etc.\ndef view_images(image): #function to view an image\n    image_cv = cv2.imread(image) #reads an image\n    plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB)); #displays an image\nview_images(Data[5])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a672e046dce3b7eb9d08d50ab84c51e77a608a91"},"cell_type":"markdown","source":"> ***Code Conclusion :*** We can see that images are very small, though they are cropped images, its hard for human eye to understand them without using some high costly machines. "},{"metadata":{"_uuid":"a893bb680a5d09a515a7667e2674301854740c73","trusted":true},"cell_type":"code","source":"def plot_images(photos) : #to plot multiple image\n    x=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        plt.subplot(5, 5, x+1)\n        plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB));\n        plt.axis('off');\n        x+=1\nplot_images(Data[:25])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f8aabaa38c1a50254707db7fef47b3a3c5a0377f"},"cell_type":"markdown","source":"Now lets look at the color ranges that our images have"},{"metadata":{"_uuid":"839444bac11b227af1fba99858b48153a750e5e6","trusted":true},"cell_type":"code","source":"def hist_plot(image): #to plot histogram of pixel values present in an image VS intensities\n    img = cv2.imread(image)\n    plt.subplot(2, 2,1)\n    view_images(image)\n    plt.subplot(2, 2,2)\n    plt.hist(img.ravel()) \n    plt.xlabel('Pixel Values')\n    plt.ylabel('Intensity')\nhist_plot(Data[29])\n    ","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e8c26432e848dcfceadc5fffd30e5957829e75c5"},"cell_type":"markdown","source":"> ***Code Conclusion :*** From the above image we can conclude that brighter region is more than the darken region in our image.  "},{"metadata":{"_uuid":"7c6d928a4c2dd217eacbce80679bfb3e21631585"},"cell_type":"markdown","source":"Next step is we need to extract the class names in which each files belong from its file names. We will save it in output.csv file."},{"metadata":{"_uuid":"fd265e222b30dbb9b5d7395a7bcd972c61121a4c","trusted":true},"cell_type":"code","source":"from tqdm import tqdm\nimport csv #to open and write csv files\nData_output=list()\nData_output.append([\"Classes\"])\nfor file_name in tqdm(Data):\n    Data_output.append([file_name[-10:-4]])\nwith open(\"output.csv\", \"w\") as f:\n    writer = csv.writer(f)\n    for val in Data_output:\n        writer.writerows([val])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e612b66edd46c1fd3dd4be271803ef516a110bbb"},"cell_type":"markdown","source":"Below code reads the data from output.csv and displays it"},{"metadata":{"_uuid":"4aaf1af0ce13cec8530e95783155c0eb9eb53eb5","trusted":true},"cell_type":"code","source":"from IPython.display import display # Allows the use of display() for DataFrames\ndata_output = pd.read_csv(\"output.csv\")\ndisplay(data_output.head(5))\nprint(data_output.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"36a676d4144f64b7a8985c15a9433b8117df9236"},"cell_type":"markdown","source":"> *Class1* represents** IDC(+)** and* Class0* represents** IDC(-)**"},{"metadata":{"_uuid":"ce678c582dbba33b36a3d4af730d2e10cc3c0596","trusted":true},"cell_type":"code","source":"def class_output(images,x,i):  #to display image along with their labels\n    fig = plt.figure()\n    ax = plt.subplot(2, 2,i)\n    ax.set_title(data_output.loc[x].item())\n    view_images(images)\n    i+=1\n    return\nk=0 #we have to show only one image of class0 therefore this variable is to check that\nl=0 #we have to show only one image of class1 therefore this variable to check that\ni=0 #for subplot position\nfor x in range(1,len(Data)):\n    if(data_output.loc[x].item()==\"class0\" and k!=1):\n        k+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(data_output.loc[x].item()==\"class1\" and l!=1):\n        l+=1\n        i+=1\n        class_output(Data[x],x,i)\n    elif(k==0 or l==0):\n        continue\n    else:\n        break","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66336def701f5917baf54ec74c0c250dad6356e4","trusted":true},"cell_type":"code","source":"def vis_data(photos,a) :\n    x=0\n    beta=0\n    for image in photos:\n        image_cv = cv2.imread(image)\n        fig=plt.figure(figsize=(50,50))\n        ax=plt.subplot(2, 5, x+1)\n        view_images(images)\n        x+=1\n        beta+=1\nplot_images(Data[0:20])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eab510947c7e73fa8c39072918aa71878d70e88b","trusted":true},"cell_type":"code","source":"class1 = data_output[(data_output[\"Classes\"]==\"class1\" )].shape[0]\nclass0 = data_output[(data_output[\"Classes\"]==\"class0\" )].shape[0]\nobjects=[\"class1\",\"class0\"]\ny_pos = np.arange(len(objects))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c41145257446bde7628e64791c3ca6bf75ca284f"},"cell_type":"markdown","source":"> ***Code Conclusion :*** We can see that we have an unbalanced class and which is a common problem when we have medical data, therefore this is one another problem that we have to deal with later."},{"metadata":{"_uuid":"9fa2c30ddd3cd41f368cba48ecc8fb175c9f0e4d","trusted":true},"cell_type":"code","source":"percent_class1=class1/len(Data)\npercent_class0=class0/len(Data)\nprint(\"Total Class1 images :\",class1)\nprint(\"Total Class0 images :\",class0)\nprint(\"Percent of class 0 images : \", percent_class0*100)\nprint(\"Percent of class 1 images : \", percent_class1*100)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3b641c8107c7d2a1553c96f03aa478d5344c01c5"},"cell_type":"markdown","source":"> ***Data Processing  *** "},{"metadata":{"_uuid":"6ef7e8eb79a1f7c7b8d0ad65b6561de1e873bccf"},"cell_type":"markdown","source":"We will first shuffle are images to remove any patterns if present and then load them."},{"metadata":{"_uuid":"5bf34dad9ab39078dee31241060fcdfea907ffb7","trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle #to shuffle the data\nData,data_output= shuffle(Data,data_output)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2aa3c9963d7d40308de1f77371f59d6f296f364f","scrolled":true,"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\ndata=list()\nfor img in tqdm(Data):\n    image_ar = cv2.imread(img)\n    data.append(cv2.resize(image_ar,(50,50),interpolation=cv2.INTER_CUBIC))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"443c4bd40b9ccf1f05eb3079ddb665eae49f9706","trusted":true},"cell_type":"markdown","source":"We would encode our output data which is present as Class1 and Class0 to 1 and 0."},{"metadata":{"_uuid":"6845fdb75b51ee19fca710994b4b5602f336ed5b","trusted":true},"cell_type":"code","source":"data_output=data_output.replace(to_replace=\"class0\",value=0)\ndata_output=data_output.replace(to_replace=\"class1\",value=1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"66650be0909e819f6d3fed65074526ff5ffe1fba"},"cell_type":"markdown","source":"In the next step we will OneHot encode our data to better work with neural networks."},{"metadata":{"trusted":true,"_uuid":"d831cfb74c6ae8dcf7465f29ee1b9888f5af6e9f"},"cell_type":"code","source":"from keras.utils import to_categorical #to hot encode the output labels\ndata_output_encoded =to_categorical(data_output, num_classes=2)\nprint(data_output_encoded.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"29df7ae3193237952c688e3aa90c2a8354a3833f"},"cell_type":"markdown","source":"Now we will split our data into training set and testing set."},{"metadata":{"_uuid":"46d3aeba9540dbb61afa6636cf52bc79f0212a63","trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndata=np.array(data)\nX_train, X_test, Y_train, Y_test = train_test_split(data, data_output_encoded, test_size=0.3,shuffle = True)\nprint(\"Number of train files\",len(X_train))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  test_target  files\",len(Y_test))\n#del(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2bc97be2dc7c2160c168b03afe0c204b5045ddd2"},"cell_type":"markdown","source":"We have a large dataset and we will work with neural networks, therefore for better debugging we will use only a part of data, considering limited RAM and non GPU processor, this will not cost us much as we would also be using under sampling methods and image argumentation to deal with class imbalances and moderate data."},{"metadata":{"trusted":true,"_uuid":"6a8c78b7941600defca98a5ba1303163b084343e"},"cell_type":"code","source":"X_train=X_train[0:70000]\nY_train=Y_train[0:70000]\nX_test=X_test[0:20000]\nY_test=Y_test[0:20000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pos=0\nneg=0\nfor x in Y_train:\n    if(np.argmax(x)==1):\n        \n        pos+=1\n    else:\n        neg+=1\nprint(pos,neg)\nfor x in Y_test:\n    if(np.argmax(x)==1):\n        pos+=1\n    else:\n        neg+=1\nprint(pos,neg)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a9c1aa63d0a0c9ac4108b742414c921ef03bd7d"},"cell_type":"markdown","source":"We will now do undersampling, to treat our data for class imbalances. The Code inspiration for undersampling is taken from a notebook - https://www.kaggle.com/paultimothymooney/predict-idc-in-breast-cancer-histology-images"},{"metadata":{"trusted":true,"_uuid":"00d07072f0d22ca2ed725cb68094c145f1291f7f"},"cell_type":"code","source":"from keras.utils import to_categorical #to hot encode the data\nfrom imblearn.under_sampling import RandomUnderSampler #For performing undersampling\n\nX_train_shape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\nX_test_shape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\nX_train_Flat = X_train.reshape(X_train.shape[0], X_train_shape)\nX_test_Flat = X_test.reshape(X_test.shape[0], X_test_shape)\n\nrandom_US = RandomUnderSampler(ratio='auto') #Constructor of the class to perform undersampling\nX_train_RUS, Y_train_RUS = random_US.fit_sample(X_train_Flat, Y_train) #resamples the dataset\nX_test_RUS, Y_test_RUS = random_US.fit_sample(X_test_Flat, Y_test) #resamples the dataset\ndel(X_train_Flat,X_test_Flat)\n\nclass1=1\nclass0=0\n\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==1):\n        class1+=1\nfor i in range(0,len(Y_train_RUS)): \n    if(Y_train_RUS[i]==0):\n        class0+=1\nfor i in range(0,len(Y_test_RUS)): \n    if(Y_test_RUS[i]==1):\n        class1+=1\nfor i in range(0,len(Y_test_RUS)): \n    if(Y_test_RUS[i]==0):\n        class0+=1\n#For Plotting the distribution of classes\nclasses=[\"class1\",\"class0\"]\ny_pos = np.arange(len(classes))\ncount=[class1,class0]\nplt.bar(y_pos, count, align='center', alpha=0.5)\nplt.xticks(y_pos, objects)\nplt.ylabel('Number of images')\nplt.title('Class distribution')\n \nplt.show()\n\n\n#hot encoding them\nY_train_encoded = to_categorical(Y_train_RUS, num_classes = 2)\nY_test_encoded = to_categorical(Y_test_RUS, num_classes = 2)\n\ndel(Y_train_RUS,Y_test_RUS)\n\nfor i in range(len(X_train_RUS)):\n    X_train_RUS_Reshaped = X_train_RUS.reshape(len(X_train_RUS),50,50,3)\ndel(X_train_RUS)\n\nfor i in range(len(X_test_RUS)):\n    X_test_RUS_Reshaped = X_test_RUS.reshape(len(X_test_RUS),50,50,3)\ndel(X_test_RUS)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a141413764d924f87aabd4846dfa3a0ca1f48be7"},"cell_type":"markdown","source":"We also need a validation set inorder to check overfitting. We can do two things either split test set further into valid set or split train se into valid set."},{"metadata":{"trusted":true},"cell_type":"code","source":"count=[class1,class0]\nprint(count)\ndel(data)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28efd4e82e1613ba589c2b5c44abb9d37e0829b0"},"cell_type":"markdown","source":"We will go for spliting testing set into validation set."},{"metadata":{"_uuid":"5d8045ba6b743225952d6f4f939f710738d3c49a","trusted":true},"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid = train_test_split(X_train_RUS_Reshaped, Y_train_encoded, test_size=0.20,shuffle=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\nX_test,Y_test= shuffle(X_test_RUS_Reshaped,Y_test_encoded)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d8442531bb4df813876ee3b793d6dac581c6111","trusted":true},"cell_type":"code","source":"print(\"Number of train files\",len(X_train))\nprint(\"Number of valid files\",len(X_valid))\nprint(\"Number of train_target files\",len(Y_train))\nprint(\"Number of  valid_target  files\",len(Y_valid))\nprint(\"Number of test files\",len(X_test))\nprint(\"Number of  test_target  files\",len(Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c442c876250c9b1648b32069997063c1b4de250e"},"cell_type":"code","source":"del(X_test_RUS_Reshaped)\ndel(X_train_RUS_Reshaped)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"eb4ca971641c91c31a891f93626ca50ed1bd9ea9"},"cell_type":"markdown","source":"> We need to now preprocess our image file. We change pixels range from 0-255 to 0-1."},{"metadata":{"_uuid":"724b481a9ec87d1206c1b827ed8daad48293e23a","trusted":true},"cell_type":"code","source":"display(Y_train_encoded.shape)\ndisplay(Y_test.shape)\ndisplay(Y_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"28a70cfa79fbf9e735ce879e09dfd3b0f7160415","trusted":true},"cell_type":"code","source":"print(\"Training Data Shape:\", X_train.shape)\nprint(\"Validation Data Shape:\", X_valid.shape)\nprint(\"Testing Data Shape:\", X_test.shape)\nprint(\"Training Label Data Shape:\", Y_train.shape)\nprint(\"Validation Label Data Shape:\", Y_valid.shape)\nprint(\"Testing Label Data Shape:\", Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1a0fabfa22a0f1f27a75b21c3ded43ef69cb8923"},"cell_type":"markdown","source":"Now we have our three sets of train, valid and test. We will now create our benchmark model."},{"metadata":{"trusted":true,"_uuid":"ac561b4edf9fb4e603011220d340aa33bdafdf10"},"cell_type":"code","source":"import itertools #create iterators for effective looping\n#Plotting the confusion matrix for checking the accuracy of the model\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = X_test.astype('float32')/255.0\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bcf108aabff562b7ec9c98e03215615d1f1de2fd"},"cell_type":"markdown","source":"\n***Image Argumentation***"},{"metadata":{"_uuid":"abe58283baa85b5507791165f8f1bec120ac90d6"},"cell_type":"markdown","source":"We will now add image argumentation to our data, so that it may be set for wider range of domain"},{"metadata":{"_uuid":"cd1f544af1e92235d162b0e5cb1148ccdcb99d87"},"cell_type":"markdown","source":"We will also rescale our image pixels, from range of 0-255.0 to 0-1."},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras_efficientnets\n#!pip install keras_squeezenet","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.xception import Xception\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.vgg19 import preprocess_input as preprocess_input19\nfrom keras.applications.vgg16 import preprocess_input as preprocess_input16\nfrom keras.applications.xception import preprocess_input as preprocess_inputx\nfrom keras.applications.mobilenet import preprocess_input as preprocess_inputm\nfrom keras.applications.resnet50 import preprocess_input as preprocess_inputr\nfrom keras.applications.densenet import DenseNet169\nfrom keras.applications.densenet  import preprocess_input as preprocess_inputD\nfrom keras.applications.inception_v3 import InceptionV3\n#from keras.applications.xception import Xception\nt_arg_model = None\nfrom keras_efficientnets import EfficientNetB5\n\n\n\n\n# loading pretrained conv base model\n\ndef model(model_name):\n    global t_arg_model\n    if(model_name=='VGG16'):\n        t_arg_model = VGG16(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='VGG19'):\n        t_arg_model = VGG19(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='resnet'):\n        t_arg_model = ResNet50(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='dense'):\n        t_arg_model = DenseNet169(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='mobile'):\n        t_arg_model = MobileNet(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='efficient'):\n        t_arg_model = EfficientNetB5(include_top=False, weights='imagenet', input_shape=(50,50,3))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model('efficient')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3d43ed66a2e1e82d7de0ecdd71430fc18551a72e","trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255) \ntrain_generator = train_datagen.flow(X_train, Y_train, \nbatch_size=32) \nval_generator = val_datagen.flow(X_valid, \nY_valid, \nbatch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=t_arg_model.predict(np.expand_dims(X_train[0],axis=0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f52daa005965b44fba0ae251220f8014e868f83"},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, BatchNormalization\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_transfer = Sequential()\nmodel_transfer.add(t_arg_model )\nmodel_transfer.add(GlobalAveragePooling2D(input_shape=x.shape[1:]))\nmodel_transfer.add(BatchNormalization(epsilon=1e-05, momentum=0.1))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.5))\n'''model_transfer.add(Dense(256,activation='relu'))\nmodel_transfer.add(Dropout(0.35))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.45))'''\n\nmodel_transfer.add(Dense(2, activation='softmax'))\n\nmodel_transfer.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7d22bb93913d3f23b4e00766534a138556e83aca"},"cell_type":"code","source":"model_transfer.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c507235810359818548f8a136863ce565119d6ac"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.effi2_batch.bestarg.hdf5', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f353ac1b8e76ec937322e53e88fe63b1bafe32fb"},"cell_type":"code","source":"model_transfer.fit_generator(train_generator, steps_per_epoch=100, \n                              epochs=50, \n                              validation_data=val_generator, \n                              validation_steps=50, \n                              callbacks=[checkpointer],\n                              verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"84c3246d5083ec1be93a78285cbadef50c7ead14"},"cell_type":"code","source":"model_transfer.load_weights('weights.effi2_batch.bestarg.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"912f75b9b0a9eea6b9a330b4796a52c3e31950b5"},"cell_type":"code","source":"predictions_arg = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test)]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d65654e5c953dbd0fe29e246ed637f96fa78b04e"},"cell_type":"markdown","source":"> ***Now we will plot the confusion matrix :***"},{"metadata":{"trusted":true,"_uuid":"5d26a31a081a8bcc7438914e923306d967630a78"},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_arg))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names,\n                      title='Confusion matrix with data argumentation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Senstivity of models\nconfusion_transfer_s=cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[0][1] + cnf_matrix_Arg[1][1]) *100\nprint(\"Senstivity :\",confusion_transfer_s)\nconfusion_transfer=cnf_matrix_Arg[0][0]/(cnf_matrix_Arg[0][0] + cnf_matrix_Arg[1][0]) *100\nprint(\"Specificity :\",confusion_transfer)\nacc = (cnf_matrix_Arg[0][0]+cnf_matrix_Arg[1][1])/(cnf_matrix_Arg[0][0] + cnf_matrix_Arg[0][1] + cnf_matrix_Arg[1][1] + cnf_matrix_Arg[1][0]) *100\nprint(\"Accuracy :\",acc)\nprec = cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[1][1] + cnf_matrix_Arg[1][0]) *100\nprint(\"Precision :\",prec)\nrecall = cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[1][1] + cnf_matrix_Arg[0][1]) *100\nprint(\"Recall:\",recall)\nprint(\"F-Score :\", 2 * (prec*recall)/(prec+recall))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"dbd4068d79d6f803a7b74a60644d24aa887d935a"},"cell_type":"markdown","source":"**Transfer Learning**"},{"metadata":{"_uuid":"86dfee2e89a8eddfcaec06824c67f91050afb167"},"cell_type":"markdown","source":"We will now add transfer learning from various models"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.image import ImageDataGenerator  #For Image argumentaton\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nval_datagen = ImageDataGenerator(rescale=1./255) \ntrain_generator = train_datagen.flow(X_train, Y_train, \nbatch_size=32) \nval_generator = val_datagen.flow(X_valid, \nY_valid, \nbatch_size=32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.applications.vgg16 import VGG16\nfrom keras.applications.vgg19 import VGG19\nfrom keras.applications.xception import Xception\nfrom keras.applications.mobilenet import MobileNet\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.applications.densenet import DenseNet121\nfrom keras.applications.vgg19 import preprocess_input as preprocess_input19\nfrom keras.applications.vgg16 import preprocess_input as preprocess_input16\nfrom keras.applications.xception import preprocess_input as preprocess_inputx\nfrom keras.applications.mobilenet import preprocess_input as preprocess_inputm\nfrom keras.applications.resnet50 import preprocess_input as preprocess_inputr\nfrom keras.applications.densenet import preprocess_input  as preprocess_inputd\narg_model = None\nbottleneck_train = None\nbottleneck_valid = None\nbottleneck_test = None\ndef model(model_name):\n    global arg_model\n    global bottleneck_train, bottleneck_valid, bottleneck_test\n    if(model_name=='VGG16'):\n        arg_model = VGG16(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='VGG19'):\n        arg_model = VGG19(include_top=False,weights='imagenet',input_shape=(50,50,3))\n        '''bottleneck_train=arg_model.predict(preprocess_input19(X_train),batch_size=50,verbose=1)\n        bottleneck_valid=arg_model.predict(preprocess_input19(X_valid),batch_size=50,verbose=1)\n        bottleneck_test=arg_model.predict(preprocess_input19(X_test),batch_size=50,verbose=1)'''\n    elif(model_name=='MobileNet'):\n        arg_model = MobileNet(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='Xception'):\n        arg_model = Xception(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='resnet'):\n        arg_model = ResNet50(include_top=False,weights='imagenet',input_shape=(50,50,3))\n    elif(model_name=='dense'):\n        arg_model = DenseNet121(include_top=False,weights='imagenet',input_shape=(50,50,3))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model('VGG16')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e82e122eebc1c3aebf2e251ecf2b07a2f0ed965b","trusted":true},"cell_type":"code","source":"x=arg_model.predict(np.expand_dims(X_train[0],axis=0))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1fe64507d712f745fb3349d5ebd1e1cbda35fc9d","trusted":true},"cell_type":"code","source":"from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nmodel_transfer = Sequential()\nmodel_transfer.add(arg_model)\nmodel_transfer.add(GlobalAveragePooling2D(input_shape=x.shape[1:]))\nmodel_transfer.add(Dense(32,activation='relu'))\nmodel_transfer.add(Dropout(0.15))\nmodel_transfer.add(Dense(64,activation='relu'))\nmodel_transfer.add(Dropout(0.20))\n'''model_transfer.add(Dense(256,activation='relu'))\nmodel_transfer.add(Dropout(0.35))\nmodel_transfer.add(Dense(512,activation='relu'))\nmodel_transfer.add(Dropout(0.45))'''\n\nmodel_transfer.add(Dense(2, activation='softmax'))\n\nmodel_transfer.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b9d9a025e6c18b3e336992fd0662729891607b55"},"cell_type":"code","source":"model_transfer.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0022e6eb98c97e20421ab9af995e25fe967258fb"},"cell_type":"code","source":"from keras.callbacks import ModelCheckpoint\ncheckpointer = ModelCheckpoint(filepath='weights.bestarg.tranfer.hdf5', verbose=1, save_best_only=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"997f8f510fc9fe90d26c7f67725fa3e79c30e90d"},"cell_type":"code","source":"batch_size=8\nepochs=50\nmodel_transfer.fit_generator(train_generator, steps_per_epoch=100, \n                              epochs=100, \n                              validation_data=val_generator, \n                              validation_steps=50, \n                              callbacks=[checkpointer],\n                              verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"735c93a916102f5daa108f2592ca39ad5db016de"},"cell_type":"code","source":"model_transfer.load_weights('weights.bestarg.tranfer.hdf5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions_transfer = [np.argmax(model_transfer.predict(np.expand_dims(feature, axis=0))) for feature in tqdm(X_test)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nclass_names=['IDC(-)','IDC(+)']\ncnf_matrix_Arg=confusion_matrix(np.argmax(Y_test, axis=1), np.array(predictions_transfer))\nplot_confusion_matrix(cnf_matrix_Arg, classes=class_names,\n                      title='Confusion matrix with data argumentation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"96ed3982b2163e18d5858f60b21f4ee2250e9011"},"cell_type":"code","source":"#Senstivity of models\nconfusion_transfer_s=cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[0][1] + cnf_matrix_Arg[1][1]) *100\nprint(\"Senstivity :\",confusion_transfer_s)\nconfusion_transfer=cnf_matrix_Arg[0][0]/(cnf_matrix_Arg[0][0] + cnf_matrix_Arg[1][0]) *100\nprint(\"Specificity :\",confusion_transfer)\nacc = (cnf_matrix_Arg[0][0]+cnf_matrix_Arg[1][1])/(cnf_matrix_Arg[0][0] + cnf_matrix_Arg[0][1] + cnf_matrix_Arg[1][1] + cnf_matrix_Arg[1][0]) *100\nprint(\"Accuracy :\",acc)\nprec = cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[1][1] + cnf_matrix_Arg[1][0]) *100\nprint(\"Precision :\",prec)\nrecall = cnf_matrix_Arg[1][1]/(cnf_matrix_Arg[1][1] + cnf_matrix_Arg[0][1]) *100\nprint(\"Recall:\",recall)\nprint(\"F-Score :\", 2 * (prec*recall)/(prec+recall))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}